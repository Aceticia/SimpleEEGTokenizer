# @package _global_
training:
  # Optimizer parameters
  learning_rate: 0.001
  weight_decay: 0.00001
  
  # Scheduler parameters
  warmup_epochs: 10
  max_epochs: 100
  warmup_start_lr: 0.000001
  eta_min: 0.000001
  
  # Logging parameters
  log_every_n_steps: 50
  checkpoint_every_n_steps: 1000